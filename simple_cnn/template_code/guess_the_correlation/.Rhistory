install.packages("luz")
library(torch)
library(luz)
torch_tensor(1)
install.packages("torchvision")
remotes::install_github("mlverse/torchdatasets")
trn_idx <- 1:10000
val_idx <- 10001:15000
tst_idx <- 15001:20000
add_channel_dim <- function(img) img$unsqueeze(1)
crop_axes <- function(img) transform_crop(img, top = 0, left = 21, height = 131, width = 130)
root <- file.path(tempdir(), "correlation")
train_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = trn_idx,
download = TRUE
)
library(torchdatasets)
trn_idx <- 1:10000
val_idx <- 10001:15000
tst_idx <- 15001:20000
add_channel_dim <- function(img) img$unsqueeze(1)
crop_axes <- function(img) transform_crop(img, top = 0, left = 21, height = 131, width = 130)
root <- file.path(tempdir(), "correlation")
train_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = trn_idx,
download = TRUE
)
valid_ds <- guess_the_correlation_dataset(
root = root,
tarnsform = function(img) crop_axes(img) |> add_channel_cim(),
indexes = val_idx,
download = FALSE
)
valid_ds <- guess_the_correlation_dataset(
root = root,
tarnsform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = val_idx,
download = FALSE
)
valid_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = val_idx,
download = FALSE
)
test_ds <- guess_the_correlation_dataset(
root = root,
tarnsform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = tst_idx,
download = FALSE
)
valid_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = val_idx,
download = FALSE
)
test_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = tst_idx,
download = FALSE
)
length(train_ds)
length(val_ds)
length(train_ds)
length(valid_ds)
length(test_ds)
train_ds[1]
train_ds[1]
library(torchvision)
train_ds[1]
train_dl <- dataloader(train_ds, batch_size = 64, shuffle = TRUE)
library(torch)
library(luz)
library(torchvision)
library(torchdatasets)
trn_idx <- 1:10000
val_idx <- 10001:15000
tst_idx <- 15001:20000
add_channel_dim <- function(img) img$unsqueeze(1)
crop_axes <- function(img) transform_crop(img, top = 0, left = 21, height = 131, width = 130)
root <- file.path(tempdir(), "correlation")
train_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = trn_idx,
download = TRUE
)
valid_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = val_idx,
download = FALSE
)
test_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = tst_idx,
download = FALSE
)
length(train_ds)
length(valid_ds)
length(test_ds)
train_ds[1]
train_dl <- dataloader(train_ds, batch_size = 64, shuffle = TRUE)
length(train_dl)
batch <- dataloader_make_iter(train_dl) |> dataloader_next()
dim(batch$x)
dim(batch$y)
dim(batch$x)
# dim(batch$x)
dim(batch$y)
par(mfrow = c(8, 8), mar = rep(0, 4))
images <- as.array(batch$x$squeeze(2))
images |>
purrr::array_tree(1) |>
purrr::map(as.raster) |>
purrr::iwalk(~{plot(.x)})
batch$y |> as.numeric() |> round(digits = 2)
valid_dl <- dataloader(valid_ds, batch_size = 64)
length(valid_dl)
test_dl <- dataloader(test_ds, batch_size = 64)
length(test_dl)
torch_manual_seed(777)
corr_mod <- nn_module(
"corr-cnn",
initialize = function() {
self$conv1 <- n_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3)
self$conv2 <- nn_conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)
self$conv3 <- nn_conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)
self$fc1 <- nn_linear(in_features = 14 * 14 * 128, out_features = 128)
self$fc2 <- nn_linear(in_features = 128, out_features = 1)
},
forward = function(x) {
x |>
self$conv1() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv2() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv3() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
torch_flatten(start_dim = 2) |>
self$fc1() |>
nnf_relu() |>
self$fc2()
}
)
torch_manual_seed(777)
corr_cnn <- nn_module(
"corr-cnn",
initialize = function() {
self$conv1 <- n_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3)
self$conv2 <- nn_conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)
self$conv3 <- nn_conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)
self$fc1 <- nn_linear(in_features = 14 * 14 * 128, out_features = 128)
self$fc2 <- nn_linear(in_features = 128, out_features = 1)
},
forward = function(x) {
x |>
self$conv1() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv2() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv3() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
torch_flatten(start_dim = 2) |>
self$fc1() |>
nnf_relu() |>
self$fc2()
}
)
mod <- corr_cnn()
self$conv1 <- nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3)
torch_manual_seed(777)
corr_cnn <- nn_module(
"corr-cnn",
initialize = function() {
self$conv1 <- nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3)
self$conv2 <- nn_conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)
self$conv3 <- nn_conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)
self$fc1 <- nn_linear(in_features = 14 * 14 * 128, out_features = 128)
self$fc2 <- nn_linear(in_features = 128, out_features = 1)
},
forward = function(x) {
x |>
self$conv1() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv2() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv3() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
torch_flatten(start_dim = 2) |>
self$fc1() |>
nnf_relu() |>
self$fc2()
}
)
mod <- corr_cnn()
model(batch$x)
mod(batch$x)
fitted <- net |>
setup(
loss = function(y_hat, y_true) nnf_mse_loss(y_hat, y_true$unsqueeze(2)),
optimizer = optim_adam
) |>
fit(train_dl, epochs = 10, valid_data = test_dl)
rm(list = ls())
library(torch)
library(luz)
library(torchvision)
library(torchdatasets)
trn_idx <- 1:10000
val_idx <- 10001:15000
tst_idx <- 15001:20000
add_channel_dim <- function(img) img$unsqueeze(1)
crop_axes <- function(img) transform_crop(img, top = 0, left = 21, height = 131, width = 130)
root <- file.path(tempdir(), "correlation")
train_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = trn_idx,
download = FALSE
)
valid_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = val_idx,
download = FALSE
)
test_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = tst_idx,
download = FALSE
)
length(train_ds)
length(valid_ds)
length(test_ds)
train_ds[1]
train_dl <- dataloader(train_ds, batch_size = 64, shuffle = TRUE)
length(train_dl)
batch <- dataloader_make_iter(train_dl) |> dataloader_next()
dim(batch$x)
dim(batch$y)
par(mfrow = c(8, 8), mar = rep(0, 4))
images <- as.array(batch$x$squeeze(2))
images |>
purrr::array_tree(1) |>
purrr::map(as.raster) |>
purrr::iwalk(~{plot(.x)})
batch$y |> as.numeric() |> round(digits = 2)
valid_dl <- dataloader(valid_ds, batch_size = 64)
length(valid_dl)
test_dl <- dataloader(test_ds, batch_size = 64)
length(test_dl)
torch_manual_seed(777)
corr_cnn <- nn_module(
"corr-cnn",
initialize = function() {
self$conv1 <- nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3)
self$conv2 <- nn_conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)
self$conv3 <- nn_conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)
self$fc1 <- nn_linear(in_features = 14 * 14 * 128, out_features = 128)
self$fc2 <- nn_linear(in_features = 128, out_features = 1)
},
forward = function(x) {
x |>
self$conv1() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv2() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv3() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
torch_flatten(start_dim = 2) |>
self$fc1() |>
nnf_relu() |>
self$fc2()
}
)
mod <- corr_cnn()
mod(batch$x)
# # run the model to obtain its current predictions on the output
# output <- model(batch$x)
#
# # evaluate how bad the predictions are right now
# loss <- nnf_mse_loss(output, batch$y$unsqueeze(2))
#
# # feed this information backward through the network, computing how changes to the weights would make the loss smaller
# loss$backward()
#
# # change the weights using the optimizer
# optimizer$step()
fitted <- mod |>
setup(
loss = function(y_hat, y_true) nnf_mse_loss(y_hat, y_true$unsqueeze(2)),
optimizer = optim_adam
) |>
fit(train_dl, epochs = 10, valid_data = test_dl)
fitted <- corr_cnn |>
setup(
loss = function(y_hat, y_true) nnf_mse_loss(y_hat, y_true$unsqueeze(2)),
optimizer = optim_adam
) |>
fit(train_dl, epochs = 10, valid_data = test_dl)
preds <- preds$to(device = "cpu")$squeeze() %>% as.numeric()
preds <- predict(fitted, test_dl)
preds <- preds$to(device = "cpu")$squeeze() %>% as.numeric()
test_dl <- dataloader(test_ds, batch_size = 5000)
targets <- (test_dl %>% dataloader_make_iter() %>% dataloader_next())$y %>% as.numeric()
df <- data.frame(preds = preds, targets = targets)
library(ggplot2)
ggplot(df, aes(x = targets, y = preds)) +
geom_point(size = 0.1) +
theme_classic() +
xlab("true correlations") +
ylab("model predictions")
library(torch)
library(luz)
library(torchvision)
library(torchdatasets)
trn_idx <- 1:10000
val_idx <- 10001:15000
tst_idx <- 15001:20000
add_channel_dim <- function(img) img$unsqueeze(1)
crop_axes <- function(img) transform_crop(img, top = 0, left = 21, height = 131, width = 130)
root <- file.path(tempdir(), "correlation")
train_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = trn_idx,
download = FALSE
)
library(torch)
library(luz)
library(torchvision)
library(torchdatasets)
trn_idx <- 1:10000
val_idx <- 10001:15000
tst_idx <- 15001:20000
add_channel_dim <- function(img) img$unsqueeze(1)
crop_axes <- function(img) transform_crop(img, top = 0, left = 21, height = 131, width = 130)
root <- file.path(tempdir(), "correlation")
train_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = trn_idx,
download = TRUE
)
valid_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = val_idx,
download = FALSE
)
test_ds <- guess_the_correlation_dataset(
root = root,
transform = function(img) crop_axes(img) |> add_channel_dim(),
indexes = tst_idx,
download = FALSE
)
length(train_ds)
length(valid_ds)
length(test_ds)
train_ds[1]
train_dl <- dataloader(train_ds, batch_size = 64, shuffle = TRUE)
length(train_dl)
batch <- dataloader_make_iter(train_dl) |> dataloader_next()
dim(batch$x)
dim(batch$y)
par(mfrow = c(8, 8), mar = rep(0, 4))
images <- as.array(batch$x$squeeze(2))
images |>
purrr::array_tree(1) |>
purrr::map(as.raster) |>
purrr::iwalk(~{plot(.x)})
batch$y |> as.numeric() |> round(digits = 2)
valid_dl <- dataloader(valid_ds, batch_size = 64)
length(valid_dl)
test_dl <- dataloader(test_ds, batch_size = 64)
length(test_dl)
torch_manual_seed(777)
corr_cnn <- nn_module(
"corr-cnn",
initialize = function() {
self$conv1 <- nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3)
self$conv2 <- nn_conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)
self$conv3 <- nn_conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)
self$fc1 <- nn_linear(in_features = 14 * 14 * 128, out_features = 128)
self$fc2 <- nn_linear(in_features = 128, out_features = 1)
},
forward = function(x) {
x |>
self$conv1() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv2() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
self$conv3() |>
nnf_relu() |>
nnf_avg_pool2d(2) |>
torch_flatten(start_dim = 2) |>
self$fc1() |>
nnf_relu() |>
self$fc2()
}
)
mod <- corr_cnn()
mod(batch$x)
# # run the model to obtain its current predictions on the output
# output <- model(batch$x)
#
# # evaluate how bad the predictions are right now
# loss <- nnf_mse_loss(output, batch$y$unsqueeze(2))
#
# # feed this information backward through the network, computing how changes to the weights would make the loss smaller
# loss$backward()
#
# # change the weights using the optimizer
# optimizer$step()
