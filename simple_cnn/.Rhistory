# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
ds = dataset("example",
initialize = function() self$iris = iris[, -5],
.getitem = function(i) list(x = torch_tensor(as.numeric(self$iris[i, ]))),
.length = function() nrow(self$iris)
)()
ds[[1]]
ds[[1]]$unsqueeze(1)
ds$.getitem(1)$unsqueeze(1)
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
return(list(x = x, id = id))
# get the data, if necessary
# set up datasets and dataloaders
library(torch)
library(torchvision)
library(torchdatasets)
trn_idx <- 1:10000
val_idx <- 10001:15000
tst_idx <- 15001:20000
add_channel_dim <- function(img) img$unsqueeze(1)
crop_axes <- function(img) transform_crop(img, top = 0, left = 21, height = 131, width = 130)
root <- file.path("data", "correlation")
# train_torch_ds <- guess_the_correlation_dataset(
#   root = root,
#   transform = function(img) add_channel_dim(crop_axes(img)),
#   indexes = trn_idx,
#   download = FALSE # change if necessary
# )
# valid_torch_ds <- guess_the_correlation_dataset(
#   root = root,
#   transform = function(img) add_channel_dim(crop_axes(img)),
#   indexes = val_idx,
#   download = FALSE
# )
# test_torch_ds <- guess_the_correlation_dataset(
#   root = root,
#   transform = function(img) add_channel_dim(crop_axes(img)),
#   indexes = tst_idx,
#   download = FALSE
# )
# train_dl <- dataloader(train_torch_ds, batch_size = 64, shuffle = TRUE)
# valid_dl <- dataloader(valid_torch_ds, batch_size = 64)
# test_dl <- dataloader(test_torch_ds, batch_size = 64)
maybe_download <- function(url, root, name, extract_fun, download) {
data_path <- fs::path_expand(fs::path(root, name))
if (!fs::dir_exists(data_path) && download) {
tmp <- tempfile()
download_file(url, tmp)
fs::dir_create(fs::path_dir(data_path), recurse = TRUE)
extract_fun(tmp, data_path)
}
if (!fs::dir_exists(data_path))
stop("No data found. Please use `download = TRUE`.")
data_path
}
guess_the_correlation_dataset_ <- torch::dataset(
"GuessTheCorrelation",
initialize = function(root, split = "train", transform = NULL, target_transform = NULL, indexes = NULL, download = FALSE) {
self$transform <- transform
self$target_transform <- target_transform
# donwload ----------------------------------------------------------
data_path <- maybe_download(
root = root,
name = "guess-the-correlation",
url = "https://storage.googleapis.com/torch-datasets/guess-the-correlation.zip",
download = download,
extract_fun = function(temp, data_path) {
unzip2(temp, exdir = data_path)
unzip2(fs::path(data_path, "train_imgs.zip"), exdir = data_path)
unzip2(fs::path(data_path, "test_imgs.zip"), exdir = data_path)
}
)
# variavel resposta -------------------------------------------------
if(split == "train") {
self$images <- readr::read_csv(fs::path(data_path, "train.csv"), col_types = c("cn"))
if(!is.null(indexes)) self$images <- self$images[indexes, ]
self$.path <- file.path(data_path, "train_imgs")
} else if(split == "submission") {
self$images <- readr::read_csv(fs::path(data_path, "example_submition.csv"), col_types = c("cn"))
self$images$corr <- NA_real_
self$.path <- file.path(data_path, "test_imgs")
}
},
.getitem = function(index) {
force(index)
sample <- self$images[index, ]
id <- sample$id
x <- torchvision::base_loader(file.path(self$.path, paste0(sample$id, ".png")))
x <- torchvision::transform_to_tensor(x) %>% torchvision::transform_rgb_to_grayscale()
if (!is.null(self$transform))
x <- self$transform(x)
# y <- torch::torch_scalar_tensor(sample$corr)
# if (!is.null(self$target_transform))
#   y <- self$target_transform(y)
return(list(x = x, id = id))
},
.length = function() {
nrow(self$images)
}
)
train_mlr3torch_ds <- guess_the_correlation_dataset_(
root = root,
transform = function(img) crop_axes(img),
indexes = trn_idx,
download = TRUE # change if necessary
)
valid_mlr3torch_ds <- guess_the_correlation_dataset_(
root = root,
transform = function(img) crop_axes(img),
indexes = val_idx,
download = FALSE
)
test_mlr3torch_ds <- guess_the_correlation_dataset_(
root = root,
transform = function(img) crop_axes(img),
indexes = tst_idx,
download = FALSE
)
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
dd_gtcorr = as_data_descriptor(train_mlr3torch_ds,
dataset_shapes = list(x = c(NA, 16900L))
)
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
rm(list = ls())
source("~/mlr3_hiwi/benchmark_experiments/simple_cnn/main.R", echo=TRUE)
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
dd_gtcorr
dd_gtcorr
tsk("iris")
tsk("mnist")
tsk("mnist")$data
tsk_mnist = tsk("mnist")
tsk_mnist$data()
lt = as_lazy_tensor(dd)
lt = as_lazy_tensor(dd_gtcorr)
lt
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
library(mlr3torch)
learner_mlp = lrn("classif.mlp",
activation = nn_relu,
neurons = c(20, 20),
batch_size = 16,
epochs = 50,
device = "cpu",
optimizer = t_opt("adam", lr = 0.1),
loss = t_loss("cross_entropy"),
callbacks = t_clbk("history"),
measures_valid = msrs(c("classif.logloss", "classif.ce")),
measures_train = msrs(c("classif.acc")),
predict_type = "prob"
)
rr = resample(
task = tsk("iris"),
learner = learner_mlp,
resampling = rsmp("holdout")
)
architecture = po("torch_ingress_num") %>>%
po("nn_linear", out_features = 20) %>>%
po("nn_relu") %>>%
po("nn_head")
graph_mlp = architecture %>>%
po("torch_loss", loss = t_loss("cross_entropy")) %>>%
po("torch_optimizer", optimizer = t_opt("adam", lr = 0.1)) %>>%
po("torch_callbacks", callbacks = t_clbk("history")) %>>%
po("torch_model_classif", batch_size = 16, epochs = 50, device = "cpu")
graph_mlp
library(mlr3torch)
learner_mlp = lrn("classif.mlp",
activation = nn_relu,
neurons = c(20, 20),
batch_size = 16,
epochs = 50,
device = "cpu",
optimizer = t_opt("adam", lr = 0.1),
loss = t_loss("cross_entropy"),
callbacks = t_clbk("history"),
measures_valid = msrs(c("classif.logloss", "classif.ce")),
measures_train = msrs(c("classif.acc")),
predict_type = "prob"
)
rr = resample(
task = tsk("iris"),
learner = learner_mlp,
resampling = rsmp("holdout")
)
rr
# architecture = po("torch_ingress_num") %>>%
#   po("nn_linear", out_features = 20) %>>%
#   po("nn_relu") %>>%
#   po("nn_head")
#
# graph_mlp = architecture %>>%
#   po("torch_loss", loss = t_loss("cross_entropy")) %>>%
#   po("torch_optimizer", optimizer = t_opt("adam", lr = 0.1)) %>>%
#   po("torch_callbacks", callbacks = t_clbk("history")) %>>%
#   po("torch_model_classif", batch_size = 16, epochs = 50, device = "cpu")
#
# graph_lrn = as_learner(graph_mlp)
# graph_lrn$id = "graph_mlp"
#
# resample(
#   task = tsk("iris"),
#   learner = graph_lrn,
#   resampling = rsmp("holdout")
# )
task
rr
rr
rr$score()
architecture = po("torch_ingress_num") %>>%
po("nn_linear", out_features = 20) %>>%
po("nn_relu") %>>%
po("nn_head")
graph_mlp = architecture %>>%
po("torch_loss", loss = t_loss("cross_entropy")) %>>%
po("torch_optimizer", optimizer = t_opt("adam", lr = 0.1)) %>>%
po("torch_callbacks", callbacks = t_clbk("history")) %>>%
po("torch_model_classif", batch_size = 16, epochs = 50, device = "cpu")
graph_lrn = as_learner(graph_mlp)
graph_lrn$id = "graph_mlp"
rr_iris = resample(
task = tsk("iris"),
learner = graph_lrn,
resampling = rsmp("holdout")
)
rr_iris$score()
tsk_iris = tsk("iris")
tsk_iris
tsk_iris$col_info
tsk_mnist = tsk("mnist")
tsk_mnist
tsk_mnist$data()
pak::pak("mlr-org/mlr3torch")
rm(list = ls())
library(mlr3torch)
learner_mlp = lrn("classif.mlp",
activation = nn_relu,
neurons = c(20, 20),
batch_size = 16,
epochs = 50,
device = "cpu",
optimizer = t_opt("adam", lr = 0.1),
loss = t_loss("cross_entropy"),
callbacks = t_clbk("history"),
measures_valid = msrs(c("classif.logloss", "classif.ce")),
measures_train = msrs(c("classif.acc")),
predict_type = "prob"
)
rr = resample(
task = tsk("iris"),
learner = learner_mlp,
resampling = rsmp("holdout")
)
rr$score()
architecture = po("torch_ingress_num") %>>%
po("nn_linear", out_features = 20) %>>%
po("nn_relu") %>>%
po("nn_head")
graph_mlp = architecture %>>%
po("torch_loss", loss = t_loss("cross_entropy")) %>>%
po("torch_optimizer", optimizer = t_opt("adam", lr = 0.1)) %>>%
po("torch_callbacks", callbacks = t_clbk("history")) %>>%
po("torch_model_classif", batch_size = 16, epochs = 50, device = "cpu")
graph_lrn = as_learner(graph_mlp)
graph_lrn$id = "graph_mlp"
rr_iris = resample(
task = tsk("iris"),
learner = graph_lrn,
resampling = rsmp("holdout")
)
rr_iris$score()
tsk_mnist = tsk("mnist")
tsk_mnist
library(here)
# source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
# start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
# start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
source(here("simple_cnn", "set_up_data.R"))
# start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
train_mlr3torch_ds
train_responses
lt = as_lazy_tensor(dd_gtcorr)
# TODO: get the y var
library(data.table)
train_responses = fread(here("simple_cnn", "data/correlation/guess-the-correlation/train_responses.csv"))
# simply cbinding the lazy tensor seems slow and the print output is... strange
dt_train = cbind(train_responses, x = lt)
# print(head(dt_train))
DataBackendDataTable$new(data = dt_train, primary_key = "..id")
dt_train
# print(head(dt_train))
DataBackendDataTable$new(data = dt_train, primary_key = "id")
# print(head(dt_train))
backend = DataBackendDataTable$new(data = dt_train, primary_key = "id")
tsk_gtcorr = TaskRegr$new(id = "guess_the_corr", backend = backend, target = "corr")
dt_train
lt
lt[1]
lt[[1]]
lt = lazy_tensor(dd_gtcorr)
lt
lt[1]
lt[[1]]
lt = lazy_tensor(dd_gtcorr)
# TODO: get the y var
library(data.table)
train_responses = fread(here("simple_cnn", "data/correlation/guess-the-correlation/train_responses.csv"))
# simply cbinding the lazy tensor seems slow and the print output is... strange
dt_train = cbind(train_responses, x = lt)
dt_train
dd_gtcorr
dd_gtcorr = as_data_descriptor(train_mlr3torch_ds,
dataset_shapes = list(x = c(NA, 130, 130))
)
lt = lazy_tensor(dd_gtcorr)
lt
# TODO: get the y var
library(data.table)
train_responses = fread(here("simple_cnn", "data/correlation/guess-the-correlation/train_responses.csv"))
# simply cbinding the lazy tensor seems slow and the print output is... strange
dt_train = cbind(train_responses, x = lt)
# print(head(dt_train))
backend = DataBackendDataTable$new(data = dt_train, primary_key = "id")
tsk_gtcorr = TaskRegr$new(id = "guess_the_corr", backend = backend, target = "corr")
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
# start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
# start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
dd_gtcorr = as_data_descriptor(train_mlr3torch_ds,
dataset_shapes = list(x = c(NA, 130, 130))
)
dd_gtcorr
lt = lazy_tensor(dd_gtcorr)
lt
library(data.table)
data.table(y = runif(10000), x = lt)
tsk_regr = as_task_regr(data.table(y = runif(10000), x = lt), target = "y")
tsk_regr
tsk_regr$head
tsk_regr$head()
lt = lazy_tensor(dd_gtcorr)
library(data.table)
train_responses = fread(here("simple_cnn", "data/correlation/guess-the-correlation/train_responses.csv"))
# here, x is a list column
dt_train = cbind(train_responses, x = lt)
dt_train
train_responses
dt_train
train_responses
?cbind
class(dt_train)
dt_train = data.table(corr = train_responses[["corr"]], x = lt)
dt_train
data.table(y = runif(10000), x = lt)
train_responses[["corr"]]
dt_train = data.table(corr = train_responses[["corr"]], x = lt)
dt_train = data.table(corr = train_responses[["corr"]], x = lt)
dt_train
class(lt)
class(dt_train$x)
train_responses = fread(here("simple_cnn", "data/correlation/guess-the-correlation/train_responses.csv"))
train_responses
data.table(y = runif(10000), x = lt)
tsk_gtcorr = as_task_regr(data.table(corr = train_responses[["corr"]], x = lt), target = "corr")
tsk_gtcorr = as_task_regr(data.table(corr = train_responses$corr, x = lt), target = "corr")
train_responses[["corr"]]
typeof(train_responses[["corr"]])
y = runif(10000)
typeof(y)
head(y)
train_responses[["corr"]] |> typeof()
train_responses[["corr"]]
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
# start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
lt
length(train_responses)
nrow(train_responses)
nrow(lt)
length(lt)
dt_train = data.table(corr = train_responses[["corr"]][1:length(lt)], x = lt)
#
dt_train = data.table(corr = train_responses[["corr"]][1:length(lt)], x = lt)
#
tsk_gtcorr = as_task_regr(dt_train, target = "corr")
tsk_gtcorr
rep
?rep
rep(lt)
rep(lt, times = 2)
lt_rep = rep(lt, times = 2)
lt_rep
View(lt_rep)
