knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
load("./data/rats_long.Rdata")
library(nlme)
modlme_rats_ri <- lme(head_length ~ time, random = ~ 1 | subject,
data = rats_long,
na.action = na.omit)
modlme_rats_ri
marginal_varcov_samerat <- getVarCov(modlme_rats_ri, type = "marginal")
marginal_varcov_samerat
marginal_varcov_samerat[[1]][1, 2] + (modlme_rats_ri$sigma ^ 2) == marginal_varcov_samerat[[1]][1, 1]
marginal_corr_samerat <- marginal_varcov_samerat[[1]] / marginal_varcov_samerat[[1]][1, 1]
marginal_corr_samerat
cond_varcov_samerat <- getVarCov(modlme_rats_ri, type = "conditional")
cond_varcov_samerat
cond_varcov_samerat_diag <- diag(cond_varcov_samerat[[1]])
# confirm that the diagonal entries are equal to the residual variance
all.equal(rep(modlme_rats_ri$sigma ^ 2, times = length(cond_varcov_samerat_diag)),
as.vector(cond_varcov_samerat_diag))
cond_corr_samerat <- cond_varcov_samerat[[1]] / cond_varcov_samerat[[1]][1, 1]
cond_corr_samerat
# helper function to count the number of measurements
n_measurements <- function(col) {
sum(!is.na(col))
}
# filter the data and perform the log transformation of time
rats_long_min3 <- rats_long |>
group_by(subject) |>
filter(n_measurements(head_length) >= 3) |>
mutate(logT = log(1 + (time - 45)/10))
rats_long_grouped <- groupedData(head_length ~ logT | subject, data = rats_long_min3)
modlist_rats_rs <- lmList(rats_long_grouped, na.action = na.omit)
modlist_rats_rs
plot(intervals(modlist_rats_rs))
modlme_rats_rs <- lme(fixed = head_length ~ logT, random = ~ logT | subject,
data = rats_long_min3,
na.action = na.omit)
modlme_rats_rs
varcov_modlme_rs <- getVarCov(modlme_rats_rs, type = "random.effects")
varcov_modlme_rs
corr_ri_rs <- varcov_modlme_rs[1, 2] / (2.0063 * 0.0002246)
plot(compareFits(coef(modlme_rats_rs), coef(modlist_rats_rs)))
comparePred(modlme_rats_rs, modlist_rats_rs,
primary = ~ logT)
comparePred(modlme_rats_rs, modlist_rats_rs)
modlme_rats_rs <- lme(fixed = head_length ~ logT, random = ~ logT | subject,
data = rats_long_grouped,
na.action = na.omit)
modlme_rats_rs
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
load("./data/rats_long.Rdata")
library(nlme)
modlme_rats_ri <- lme(head_length ~ time, random = ~ 1 | subject,
data = rats_long,
na.action = na.omit)
modlme_rats_ri
marginal_varcov_samerat <- getVarCov(modlme_rats_ri, type = "marginal")
marginal_varcov_samerat
marginal_varcov_samerat[[1]][1, 2] + (modlme_rats_ri$sigma ^ 2) == marginal_varcov_samerat[[1]][1, 1]
marginal_corr_samerat <- marginal_varcov_samerat[[1]] / marginal_varcov_samerat[[1]][1, 1]
marginal_corr_samerat
cond_varcov_samerat <- getVarCov(modlme_rats_ri, type = "conditional")
cond_varcov_samerat
cond_varcov_samerat_diag <- diag(cond_varcov_samerat[[1]])
# confirm that the diagonal entries are equal to the residual variance
all.equal(rep(modlme_rats_ri$sigma ^ 2, times = length(cond_varcov_samerat_diag)),
as.vector(cond_varcov_samerat_diag))
cond_corr_samerat <- cond_varcov_samerat[[1]] / cond_varcov_samerat[[1]][1, 1]
cond_corr_samerat
# helper function to count the number of measurements
n_measurements <- function(col) {
sum(!is.na(col))
}
# filter the data and perform the log transformation of time
rats_long_min3 <- rats_long |>
group_by(subject) |>
filter(n_measurements(head_length) >= 3) |>
mutate(logT = log(1 + (time - 45)/10))
rats_long_grouped <- groupedData(head_length ~ logT | subject, data = rats_long_min3)
modlist_rats_rs <- lmList(rats_long_grouped, na.action = na.omit)
modlist_rats_rs
plot(intervals(modlist_rats_rs))
modlme_rats_rs <- lme(fixed = head_length ~ logT, random = ~ logT | subject,
data = rats_long_grouped,
na.action = na.omit)
modlme_rats_rs
varcov_modlme_rs <- getVarCov(modlme_rats_rs, type = "random.effects")
varcov_modlme_rs
corr_ri_rs <- varcov_modlme_rs[1, 2] / (2.0063 * 0.0002246)
plot(compareFits(coef(modlme_rats_rs), coef(modlist_rats_rs)))
comparePred(modlme_rats_rs, modlist_rats_rs)
?Nlme
?nlme
modlme_rats_rs <- lme(fixed = head_length ~ logT,
data = rats_long_grouped,
na.action = na.omit)
modlme_rats_rs
, random = ~ logT | subject
modlme_rats_rs
modlme_rats_rs <- lme(fixed = head_length ~ logT,
data = rats_long_grouped,
na.action = na.omit)
modlme_rats_rs
varcov_modlme_rs <- getVarCov(modlme_rats_rs, type = "random.effects")
varcov_modlme_rs
corr_ri_rs <- varcov_modlme_rs[1, 2] / (2.0063 * 0.0002246)
plot(compareFits(coef(modlme_rats_rs), coef(modlist_rats_rs)))
comparePred(modlme_rats_rs, modlist_rats_rs)
length(modlme_rats_rs)
length(modlist_rats_rs)
comparePred(modlme_rats_rs, modlist_rats_rs,
length.out = 2)
?comparePred
predict(modlme_rats_rs)
predict(modlist_rats_rs)
comparePred(predict(modlme_rats_rs), predict(modlist_rats_rs))
comparePred(modlme_rats_rs, modlist_rats_rs)
comparePred(modlme_rats_rs, modlist_rats_rs)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
load("./data/rats_long.Rdata")
library(nlme)
modlme_rats_ri <- lme(head_length ~ time, random = ~ 1 | subject,
data = rats_long,
na.action = na.omit)
modlme_rats_ri
marginal_varcov_samerat <- getVarCov(modlme_rats_ri, type = "marginal")
marginal_varcov_samerat
marginal_varcov_samerat[[1]][1, 2] + (modlme_rats_ri$sigma ^ 2) == marginal_varcov_samerat[[1]][1, 1]
marginal_corr_samerat <- marginal_varcov_samerat[[1]] / marginal_varcov_samerat[[1]][1, 1]
marginal_corr_samerat
cond_varcov_samerat <- getVarCov(modlme_rats_ri, type = "conditional")
cond_varcov_samerat
cond_varcov_samerat_diag <- diag(cond_varcov_samerat[[1]])
# confirm that the diagonal entries are equal to the residual variance
all.equal(rep(modlme_rats_ri$sigma ^ 2, times = length(cond_varcov_samerat_diag)),
as.vector(cond_varcov_samerat_diag))
cond_corr_samerat <- cond_varcov_samerat[[1]] / cond_varcov_samerat[[1]][1, 1]
cond_corr_samerat
# helper function to count the number of measurements
n_measurements <- function(col) {
sum(!is.na(col))
}
# filter the data and perform the log transformation of time
rats_long_min3 <- rats_long |>
group_by(subject) |>
filter(n_measurements(head_length) >= 3) |>
mutate(logT = log(1 + (time - 45)/10))
rats_long_grouped <- groupedData(head_length ~ logT | subject, data = rats_long_min3)
modlist_rats_rs <- lmList(rats_long_grouped, na.action = na.omit)
modlist_rats_rs
plot(intervals(modlist_rats_rs))
modlme_rats_rs <- lme(fixed = head_length ~ logT,
data = rats_long_grouped,
na.action = na.omit)
modlme_rats_rs
varcov_modlme_rs <- getVarCov(modlme_rats_rs, type = "random.effects")
varcov_modlme_rs
corr_ri_rs <- varcov_modlme_rs[1, 2] / (2.0063 * 0.0002246)
plot(compareFits(coef(modlme_rats_rs), coef(modlist_rats_rs)))
comparePred(modlme_rats_rs, modlist_rats_rs)
modlme_rats_rs <- lme(fixed = head_length ~ logT, random = ~ logT | subject,
data = rats_long_grouped,
na.action = na.omit)
modlme_rats_rs
varcov_modlme_rs <- getVarCov(modlme_rats_rs, type = "random.effects")
varcov_modlme_rs
corr_ri_rs <- varcov_modlme_rs[1, 2] / (2.0063 * 0.0002246)
plot(compareFits(coef(modlme_rats_rs), coef(modlist_rats_rs)))
comparePred(modlme_rats_rs, modlist_rats_rs)
library(here)
source(here("simple_cnn", "set_up_data.R"))
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
rm(list = ls())
options(error = browser())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
dd_gtcorr = as_data_descriptor(train_mlr3torch_ds, dataset_shapes = list(x = c(NA, 16900L)))
dd_gtcorr = as_data_descriptor(train_mlr3torch_ds, dataset_shapes = list(x = c(NA, 16900L)))
# TODO: get the y values
tsk_gtcorr = TaskRegr$new(id = "guess_the_corr", backend = train_mlr3torch_ds, target = "y")
dd_gtcorr = as_data_descriptor(train_mlr3torch_ds, dataset_shapes = list(x = c(NA, 16900L)))
map(train_mlr3torch_ds, function(x) x$unsqueeze(1))
purrr::map(train_mlr3torch_ds, function(x) x$unsqueeze(1))
class(train_mlr3torch_ds)
train_mlr3torch_ds
train_mlr3torch_ds$.getitem(1)$unsqueeze(1)
train_mlr3torch_ds$.getitem(1)
train_mlr3torch_ds$.getitem(1)
train_mlr3torch_ds$.getitem(1)$x
train_mlr3torch_ds$.getitem(1)$x$unsqueeze(1)
train_mlr3torch_ds$.getitem(1)$x$unsqueeze(1)r
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
library(torchdatasets)
maybe_download <- function(url, root, name, extract_fun, download) {
data_path <- fs::path_expand(fs::path(root, name))
if (!fs::dir_exists(data_path) && download) {
tmp <- tempfile()
download_file(url, tmp)
fs::dir_create(fs::path_dir(data_path), recurse = TRUE)
extract_fun(tmp, data_path)
}
if (!fs::dir_exists(data_path))
stop("No data found. Please use `download = TRUE`.")
data_path
}
guess_the_correlation_dataset_ <- torch::dataset(
"GuessTheCorrelation",
initialize = function(root, split = "train", transform = NULL, target_transform = NULL, indexes = NULL, download = FALSE) {
self$transform <- transform
self$target_transform <- target_transform
# donwload ----------------------------------------------------------
data_path <- maybe_download(
root = root,
name = "guess-the-correlation",
url = "https://storage.googleapis.com/torch-datasets/guess-the-correlation.zip",
download = download,
extract_fun = function(temp, data_path) {
unzip2(temp, exdir = data_path)
unzip2(fs::path(data_path, "train_imgs.zip"), exdir = data_path)
unzip2(fs::path(data_path, "test_imgs.zip"), exdir = data_path)
}
)
# variavel resposta -------------------------------------------------
if(split == "train") {
self$images <- readr::read_csv(fs::path(data_path, "train.csv"), col_types = c("cn"))
if(!is.null(indexes)) self$images <- self$images[indexes, ]
self$.path <- file.path(data_path, "train_imgs")
} else if(split == "submission") {
self$images <- readr::read_csv(fs::path(data_path, "example_submition.csv"), col_types = c("cn"))
self$images$corr <- NA_real_
self$.path <- file.path(data_path, "test_imgs")
}
},
.getitem = function(index) {
force(index)
sample <- self$images[index, ]
id <- sample$id
x <- torchvision::base_loader(file.path(self$.path, paste0(sample$id, ".png")))
x <- torchvision::transform_to_tensor(x) %>% torchvision::transform_rgb_to_grayscale()
if (!is.null(self$transform))
x <- self$transform(x)
# y <- torch::torch_scalar_tensor(sample$corr)
# if (!is.null(self$target_transform))
#   y <- self$target_transform(y)
return(list(x = x, id = id))
},
.length = function() {
nrow(self$images)
}
)
train_mlr3torch_ds <- guess_the_correlation_dataset_(
root = root,
transform = function(img) crop_axes(img),
indexes = trn_idx,
download = TRUE # change if necessary
)
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
ds = dataset("example",
initialize = function() self$iris = iris[, -5],
.getitem = function(i) list(x = torch_tensor(as.numeric(self$iris[i, ]))),
.length = function() nrow(self$iris)
)()
ds[[1]]
ds[[1]]$unsqueeze(1)
ds$.getitem(1)$unsqueeze(1)
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
return(list(x = x, id = id))
# get the data, if necessary
# set up datasets and dataloaders
library(torch)
library(torchvision)
library(torchdatasets)
trn_idx <- 1:10000
val_idx <- 10001:15000
tst_idx <- 15001:20000
add_channel_dim <- function(img) img$unsqueeze(1)
crop_axes <- function(img) transform_crop(img, top = 0, left = 21, height = 131, width = 130)
root <- file.path("data", "correlation")
# train_torch_ds <- guess_the_correlation_dataset(
#   root = root,
#   transform = function(img) add_channel_dim(crop_axes(img)),
#   indexes = trn_idx,
#   download = FALSE # change if necessary
# )
# valid_torch_ds <- guess_the_correlation_dataset(
#   root = root,
#   transform = function(img) add_channel_dim(crop_axes(img)),
#   indexes = val_idx,
#   download = FALSE
# )
# test_torch_ds <- guess_the_correlation_dataset(
#   root = root,
#   transform = function(img) add_channel_dim(crop_axes(img)),
#   indexes = tst_idx,
#   download = FALSE
# )
# train_dl <- dataloader(train_torch_ds, batch_size = 64, shuffle = TRUE)
# valid_dl <- dataloader(valid_torch_ds, batch_size = 64)
# test_dl <- dataloader(test_torch_ds, batch_size = 64)
maybe_download <- function(url, root, name, extract_fun, download) {
data_path <- fs::path_expand(fs::path(root, name))
if (!fs::dir_exists(data_path) && download) {
tmp <- tempfile()
download_file(url, tmp)
fs::dir_create(fs::path_dir(data_path), recurse = TRUE)
extract_fun(tmp, data_path)
}
if (!fs::dir_exists(data_path))
stop("No data found. Please use `download = TRUE`.")
data_path
}
guess_the_correlation_dataset_ <- torch::dataset(
"GuessTheCorrelation",
initialize = function(root, split = "train", transform = NULL, target_transform = NULL, indexes = NULL, download = FALSE) {
self$transform <- transform
self$target_transform <- target_transform
# donwload ----------------------------------------------------------
data_path <- maybe_download(
root = root,
name = "guess-the-correlation",
url = "https://storage.googleapis.com/torch-datasets/guess-the-correlation.zip",
download = download,
extract_fun = function(temp, data_path) {
unzip2(temp, exdir = data_path)
unzip2(fs::path(data_path, "train_imgs.zip"), exdir = data_path)
unzip2(fs::path(data_path, "test_imgs.zip"), exdir = data_path)
}
)
# variavel resposta -------------------------------------------------
if(split == "train") {
self$images <- readr::read_csv(fs::path(data_path, "train.csv"), col_types = c("cn"))
if(!is.null(indexes)) self$images <- self$images[indexes, ]
self$.path <- file.path(data_path, "train_imgs")
} else if(split == "submission") {
self$images <- readr::read_csv(fs::path(data_path, "example_submition.csv"), col_types = c("cn"))
self$images$corr <- NA_real_
self$.path <- file.path(data_path, "test_imgs")
}
},
.getitem = function(index) {
force(index)
sample <- self$images[index, ]
id <- sample$id
x <- torchvision::base_loader(file.path(self$.path, paste0(sample$id, ".png")))
x <- torchvision::transform_to_tensor(x) %>% torchvision::transform_rgb_to_grayscale()
if (!is.null(self$transform))
x <- self$transform(x)
# y <- torch::torch_scalar_tensor(sample$corr)
# if (!is.null(self$target_transform))
#   y <- self$target_transform(y)
return(list(x = x, id = id))
},
.length = function() {
nrow(self$images)
}
)
train_mlr3torch_ds <- guess_the_correlation_dataset_(
root = root,
transform = function(img) crop_axes(img),
indexes = trn_idx,
download = TRUE # change if necessary
)
valid_mlr3torch_ds <- guess_the_correlation_dataset_(
root = root,
transform = function(img) crop_axes(img),
indexes = val_idx,
download = FALSE
)
test_mlr3torch_ds <- guess_the_correlation_dataset_(
root = root,
transform = function(img) crop_axes(img),
indexes = tst_idx,
download = FALSE
)
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
dd_gtcorr = as_data_descriptor(train_mlr3torch_ds,
dataset_shapes = list(x = c(NA, 16900L))
)
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
rm(list = ls())
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
rm(list = ls())
source("~/mlr3_hiwi/benchmark_experiments/simple_cnn/main.R", echo=TRUE)
library(here)
source(here("simple_cnn", "set_up_data.R"))
# start_time_torch <- proc.time()
# source("learner_torch.R")
# source("train_torch.R")
# # source("predict_torch.R")
# elapsed_time_torch <- proc.time() - start_time_torch
start_time_mlr3torch <- proc.time()
source(here("simple_cnn", "learner_mlr3torch.R"))
source(here("simple_cnn", "train_mlr3torch.R"))
dd_gtcorr
dd_gtcorr
tsk("iris")
tsk("mnist")
tsk("mnist")$data
tsk_mnist = tsk("mnist")
tsk_mnist$data()
lt = as_lazy_tensor(dd)
lt = as_lazy_tensor(dd_gtcorr)
lt
